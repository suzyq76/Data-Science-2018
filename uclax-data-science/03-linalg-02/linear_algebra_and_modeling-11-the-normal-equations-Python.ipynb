{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../preamble.py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the Normal Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given $(-3.1,4.2)$, $(-2.1,2.4)$, $(1.8,-2.5)$, $(0.5,-1.3)$, $(-1.1,1.9)$, we seek $\\beta = (\\beta_0,\\beta_1)$ so that for\n",
    "\n",
    "$$\n",
    "\\left(\\begin{matrix}1&-3.1\\\\1&-2.1\\\\1&1.8\\\\1&0.5\\\\1&-1.1\\end{matrix}\\right)\n",
    "\\left(\\begin{matrix}\\beta_0\\\\\\beta_1\\end{matrix}\\right)\\approx\\left(\\begin{matrix}4.2\\\\2.4\\\\-2.5\\\\-1.3\\\\1.9\\end{matrix}\\right)$$\n",
    "\n",
    "$$X\\beta\\approx\\mathbf{y}$$\n",
    "\n",
    "and with error \n",
    "\n",
    "$$ \\varepsilon = X\\beta  - \\mathbf{y} $$\n",
    "\n",
    "the loss function \n",
    "\n",
    "$$\\mathcal{L}(\\beta) = \\varepsilon^T \\varepsilon$$\n",
    "\n",
    "is at a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we find a minimum?\n",
    "\n",
    "Calculus and Linear Algebra!!\n",
    "\n",
    "\n",
    "##### Special Transpose Property\n",
    "We are going to need this fact: The transpose has a special property so that\n",
    "\n",
    "$$(UV)^T = V^TU^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimizing the Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\mathcal{L}(\\beta) &= \\varepsilon^T \\varepsilon\\\\\n",
    "&=(X\\beta - \\mathbf{y})^T(X\\beta - \\mathbf{y})\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "##### Expansion \n",
    "Let's expand this using FOIL.\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\beta) &=(X\\beta)^TX\\beta - \\mathbf{y}^TX\\beta  - (X\\beta)^T\\mathbf{y} + \\mathbf{y}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "##### Application of Transpose Property\n",
    "We apply the special property of the transpose to the first term\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\beta) &=\\beta^TX^T X\\beta - \\mathbf{y}^TX\\beta  - (X\\beta)^T\\mathbf{y} + \\mathbf{y}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "##### Combine middle terms using Properties of Dot Product\n",
    "\n",
    "Recall that the dot product is order **independent**. Therefore \n",
    "\n",
    "$$ \\mathbf{y}^TX\\beta  = (X\\beta)^T\\mathbf{y}$$\n",
    "\n",
    "Therefore, we can combine the middle terms which gives us\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\beta) &=\\beta^TX^T X\\beta - 2(X\\beta)^T\\mathbf{y} + \\mathbf{y}^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "##### Application of the Transpose Property\n",
    "\n",
    "We next apply the transpose property again\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\beta) &=\\beta^TX^T X\\beta - 2\\beta^TX^T\\mathbf{y} + \\mathbf{y}^T\\mathbf{y}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimize by Taking the Derivative\n",
    "To find the minimum of the loss function for a vector $\\beta$, we take the derivative with respect to $\\beta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{d}{d\\beta}\\mathcal{L}(\\beta) &= \\frac{d}{d\\beta}\\left(\\beta^TX^T X\\beta - 2\\beta^TX^T\\mathbf{y} + \\mathbf{y}^T\\mathbf{y}\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set it equal to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dx}x^b=bx^{b-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{d}{dx}x^2 = \\frac{d}{dx}x\\cdot x = 2x$$\n",
    "\n",
    "$$\\frac{d}{dx}3x = 3$$\n",
    "\n",
    "$$\\frac{d}{dx}5 = 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac{d}{d\\beta}\\mathcal{L}(\\beta) &= 2X^T X\\beta - 2X^T\\mathbf{y} = 0\\\\\n",
    "\\end{align}\n",
    "\n",
    "This leaves us with \n",
    "\n",
    "$$ X^T X\\beta = X^T\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.random.rand(10).reshape((5,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.T.dot(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall the Original Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left(\\begin{matrix}1&-3.1\\\\1&-2.1\\\\1&1.8\\\\1&0.5\\\\1&-1.1\\end{matrix}\\right)\n",
    "\\left(\\begin{matrix}\\beta_0\\\\\\beta_1\\end{matrix}\\right)=\\left(\\begin{matrix}4.2\\\\2.4\\\\-2.5\\\\-1.3\\\\1.9\\end{matrix}\\right)$$\n",
    "\n",
    "this result shows that we have a minimum loss when \n",
    "\n",
    "$$X^TX\\beta=X^T\\mathbf{y}$$\n",
    "\n",
    "which we can easily solve by \n",
    "\n",
    "$$\\beta=(X^TX)^{-1}X^T\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equations in Matrix Form\n",
    "\n",
    "This equation is known as the matrix form of the [normal equations](normeq)\n",
    "\n",
    "$$\\beta=(X^TX)^{-1}X^T\\mathbf{y}$$\n",
    "\n",
    "\n",
    "[normeq]: https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)#Inverting_the_matrix_of_the_normal_equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([(1,-3.1),(1,-2.1),(1,1.8),(1,0.5),(1,-1.1)])\n",
    "y = np.array((4.2,2.4,-2.5,-1.3,1.9))\n",
    "beta = inverse(X.T.dot(X)).dot(X.T).dot(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.T.dot(X.T).dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.T.dot(X).dot(beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check our Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = initialize_2d_plot(-5,5,-5,5)\n",
    "plt.plot(-3.1,4.2,'o',c='red', markersize=15)\n",
    "plt.plot(-2.1,2.4,'o',c='red', markersize=15)\n",
    "plt.plot(1.8,-2.5,'o',c='red', markersize=15)\n",
    "plt.plot(0.5,-1.3,'o',c='red', markersize=15)\n",
    "plt.plot(-1.1,1.9,'o',c='red', markersize=15)\n",
    "f = lambda x: beta[0] + beta[1]*x\n",
    "plt.plot(xx, f(xx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Best\" Fit\n",
    "\n",
    "It can be shown by the [Gauss-Markov theorem](https://en.wikipedia.org/wiki/Gaussâ€“Markov_theorem) that the $\\beta$ vector we found defines the \"best\" fit, that is it defines the line with the Best Linear Unbiased Estimator.\n",
    "\n",
    "<img src=\"https://s-media-cache-ak0.pinimg.com/originals/98/97/80/9897808ebe39a3098b9c3f26e9bbda1d.jpg\"\n",
    "     width=\"400px\"\n",
    "     style=\"display: block; margin: 0 auto\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
